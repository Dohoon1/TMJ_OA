import pandas as pd
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.utils import resample
    
# --- 0. ì‹œë“œ ê³ ì • ë° ì„¤ì • ---
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if using multi-GPU
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# --- 1. ë°ì´í„° ë¡œë”© ---
xlsx_path = '/content/TMJ_OA_MRI_data.xlsx'
df = pd.read_excel(xlsx_path).dropna()
df.rename(columns={'PA_TMJ_OA': 'PR_TMJ_OA'}, inplace=True)
df = df.drop(columns=['ID', 'Pain side'])

target = 'MRI_TMJ_OA'
y = df[target].values

# --- 2. ëª¨ë¸ë³„ ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ ì •ì˜ ---
features_model1 = [
    'Sex', 'Age', 'VAS', 'Symptom duration', 'TMJ noise', 
    'Muscle stiffness', 'Locking', 'Bruxism'
] # Clinical only (8ê°œ)

features_model2 = [
    'MRI_TMJ_ADD', 'PR_TMJ_OA', 'NaMx_Discrepancy', 'MxMn_Discrepancy'
] # Imaging only (4ê°œ)

features_model3 = features_model1 + features_model2 # Combined (12ê°œ)

# ì‹¤í–‰í•  ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
model_feature_sets = {
    "Model 1 (Clinical Variables only)": features_model1,
    "Model 2 (Imaging Variables only)": features_model2,
    "Model 3 (Combined Variables)": features_model3
}

# --- 3. ëª¨ë¸ ë° í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
criterion = nn.BCELoss()

def calculate_ci(y_true, y_prob, n_bootstraps=1000, alpha=0.95):
    """
    ë‹¨ì¼ Fold ê²°ê³¼ì— ëŒ€í•´ ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ìœ¼ë¡œ 95% CIë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    stats = []
    # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ë¶€íŠ¸ìŠ¤íŠ¸ë©ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆì–´ ì˜ˆì™¸ì²˜ë¦¬
    if len(y_true) < 5: 
        return 0.0, 0.0

    for i in range(n_bootstraps):
        # ë³µì› ì¶”ì¶œ
        y_true_boot, y_prob_boot = resample(y_true, y_prob, random_state=i)
        
        # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë§Œ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ (AUC ê³„ì‚° ë¶ˆê°€)
        if len(np.unique(y_true_boot)) < 2:
            continue
            
        score = roc_auc_score(y_true_boot, y_prob_boot)
        stats.append(score)
    
    if not stats: return 0.0, 0.0

    # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (Percentile ë°©ì‹)
    lower = np.percentile(stats, ((1.0 - alpha) / 2.0) * 100)
    upper = np.percentile(stats, (alpha + (1.0 - alpha) / 2.0) * 100)
    
    return lower, upper

class LogisticRegressionModel(nn.Module):
    def __init__(self, in_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(in_dim, 1), nn.Sigmoid())
    def forward(self, x): return self.net(x)

def train_fixed_epochs(model, loader, optimizer, n_epochs):
    model.train()
    for _ in range(n_epochs):
        for x_batch, y_batch in loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            # y_batchê°€ squeeze()ëœ ì˜ˆì¸¡ê³¼ ì°¨ì›ì´ ë§ë„ë¡ [B] -> [B, 1] ë˜ëŠ” squeeze()
            loss = criterion(model(x_batch).squeeze(), y_batch) 
            loss.backward(); optimizer.step()

# CI ê³„ì‚° í•¨ìˆ˜
def calculate_ci(y_true, y_prob, n_bootstraps=1000, alpha=0.95):
    stats = []
    if len(y_true) < 5: return 0.0, 0.0
    for i in range(n_bootstraps):
        y_true_boot, y_prob_boot = resample(y_true, y_prob, random_state=i)
        if len(np.unique(y_true_boot)) < 2: continue
        stats.append(roc_auc_score(y_true_boot, y_prob_boot))
    if not stats: return 0.0, 0.0
    lower = np.percentile(stats, ((1.0 - alpha) / 2.0) * 100)
    upper = np.percentile(stats, (alpha + (1.0 - alpha) / 2.0) * 100)
    return lower, upper

# ì‹œê°í™” í•¨ìˆ˜ 1: ROC Curve (Mean Â± Std)
def plot_mean_roc_with_std(tprs, aucs, model_name, ax=None, color='blue'):
    if ax is None: fig, ax = plt.subplots(figsize=(8, 6))
    mean_fpr = np.linspace(0, 1, 100)
    
    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    std_tpr = np.std(tprs, axis=0)
    
    mean_auc = np.mean(aucs)
    std_auc = np.std(aucs)
    
    ax.plot(mean_fpr, mean_tpr, color=color,
            label=f'{model_name} (AUC = {mean_auc:.3f} $\pm$ {std_auc:.3f})', lw=2, alpha=.8)
    
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=0.2, label=r'$\pm$ std. dev.')
    
    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='black', alpha=.6)
    ax.set_xlim([-0.05, 1.05]); ax.set_ylim([-0.05, 1.05])
    ax.set_xlabel('False Positive Rate'); ax.set_ylabel('True Positive Rate')
    ax.set_title(f'ROC Curve: {model_name}')
    ax.legend(loc="lower right"); ax.grid(alpha=0.3)
    return ax

# ì‹œê°í™” í•¨ìˆ˜ 2: Odds Ratio (Mean Â± Std)
def plot_odds_ratio_with_std(fold_weights, feature_names, model_name):
    """
    ê° Foldì—ì„œ í•™ìŠµëœ ê°€ì¤‘ì¹˜(Weight)ë¥¼ ëª¨ì•„ Odds Ratioì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì‹œê°í™”
    """
    # Weights -> Odds Ratios ë³€í™˜ (exp)
    odds_ratios = np.exp(np.array(fold_weights)) # shape: (n_folds, n_features)
    
    # í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
    mean_or = np.mean(odds_ratios, axis=0)
    std_or = np.std(odds_ratios, axis=0)
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì •ë ¬ìš©)
    or_df = pd.DataFrame({
        'Feature': feature_names,
        'Mean_OR': mean_or,
        'Std_OR': std_or
    }).sort_values(by='Mean_OR', ascending=True) # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ì„ ìœ„í•´ Ascending=Trueë¡œ í•œ ë’¤ barh
    
    plt.figure(figsize=(10, max(6, len(feature_names) * 0.5)))
    
    # 1 ê¸°ì¤€ì„  (Effect ì—†ìŒ)
    plt.axvline(x=1, color='red', linestyle='--', linewidth=1, label='OR=1 (No Effect)')
    
    # Bar plot with Error bars
    plt.barh(or_df['Feature'], or_df['Mean_OR'], xerr=or_df['Std_OR'], 
             capsize=5, color='skyblue', alpha=0.8, edgecolor='black')
    
    # ê°’ í…ìŠ¤íŠ¸ í‘œì‹œ
    for index, value in enumerate(or_df['Mean_OR']):
        plt.text(value + or_df['Std_OR'].iloc[index] + 0.05, index, 
                 f'{value:.2f} Â± {or_df["Std_OR"].iloc[index]:.2f}', 
                 va='center', fontsize=9)

    plt.xlabel('Odds Ratio (Mean Â± Std over 5 Folds)')
    plt.title(f'Feature Importance (Odds Ratio): {model_name}')
    plt.legend()
    plt.tight_layout()
    plt.show()

# --- 4. ì¤‘ì²© êµì°¨ ê²€ì¦ (Nested Cross-Validation) ì„¤ì • ---
LR_LIST = [0.05, 0.01, 0.005, 0.001]
EPOCH_LIST = [500, 1000, 1500, 2000]
BATCH_SIZE = 128
OUTER_K = 5  # ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ì™¸ë¶€ ë£¨í”„ K
INNER_K = 5  # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•œ ë‚´ë¶€ ë£¨í”„ K

t = lambda a: torch.tensor(a, dtype=torch.float32)

# --- 5. ëª¨ë¸ë³„ NCV ì‹¤í–‰ ë£¨í”„ ---
for model_name, current_features in model_feature_sets.items():
    
    print("\n" + "=" * 60)
    print(f"ğŸš€ [Testing] {model_name}")
    print(f"   > Features ({len(current_features)}): {', '.join(current_features)}")
    print("=" * 60)

    # í˜„ì¬ ëª¨ë¸ì˜ X ë°ì´í„° ë° input_dim ì •ì˜
    X = df[current_features].values
    input_dim = X.shape[1]

    # ì™¸ë¶€ ë£¨í”„ (ì„±ëŠ¥ í‰ê°€)
    skf_outer = StratifiedKFold(n_splits=OUTER_K, shuffle=True, random_state=42)

    # ê° í´ë“œì˜ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ (ë§¤ ëª¨ë¸ë§ˆë‹¤ ì´ˆê¸°í™”)
    outer_test_accuracy = []
    outer_test_auroc = []
    outer_test_sensitivity = []
    outer_test_specificity = []
    outer_test_f1 = []
    
    # ì‹œê°í™”ìš© ë°ì´í„° ìˆ˜ì§‘
    tprs = []
    mean_fpr = np.linspace(0, 1, 100)
    fold_weights = [] # ê° Foldì˜ í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥

    for fold_idx, (outer_train_idx, outer_test_idx) in enumerate(skf_outer.split(X, y)):
        print(f"--- Outer Fold {fold_idx + 1}/{OUTER_K} ---")
        
        X_outer_train, y_outer_train = X[outer_train_idx], y[outer_train_idx]
        X_outer_test, y_outer_test = X[outer_test_idx], y[outer_test_idx]

        # --- ë‚´ë¶€ ë£¨í”„ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹) ---
        skf_inner = StratifiedKFold(n_splits=INNER_K, shuffle=True, random_state=42)
        best_cfg, best_inner_cv_acc = None, -1.0

        for lr in LR_LIST:
            for n_ep in EPOCH_LIST:
                inner_fold_acc = []
                
                for inner_tr_idx, inner_val_idx in skf_inner.split(X_outer_train, y_outer_train):
                    X_inner_tr, X_inner_val = X_outer_train[inner_tr_idx], X_outer_train[inner_val_idx]
                    y_inner_tr, y_inner_val = y_outer_train[inner_tr_idx], y_outer_train[inner_val_idx]

                    scaler = StandardScaler().fit(X_inner_tr)
                    X_inner_tr_s = scaler.transform(X_inner_tr)
                    X_inner_val_s = scaler.transform(X_inner_val)
                    
                    if len(X_inner_tr_s) > 0 and len(np.unique(y_inner_tr)) > 1:
                        X_tr_sm, y_tr_sm = SMOTE(random_state=42).fit_resample(X_inner_tr_s, y_inner_tr)
                    else:
                        X_tr_sm, y_tr_sm = X_inner_tr_s, y_inner_tr

                    train_ld = DataLoader(TensorDataset(t(X_tr_sm), t(y_tr_sm)), batch_size=BATCH_SIZE, shuffle=True)
                    val_ld   = DataLoader(TensorDataset(t(X_inner_val_s), t(y_inner_val)), batch_size=BATCH_SIZE)

                    model = LogisticRegressionModel(input_dim).to(device)
                    opt = optim.Adam(model.parameters(), lr=lr)
                    train_fixed_epochs(model, train_ld, opt, n_ep)

                    model.eval()
                    p, l = [], []
                    with torch.no_grad():
                        for x_val, y_val_data in val_ld:
                            out = model(x_val.to(device)).squeeze().cpu().numpy()
                            p.extend((out > 0.5).astype(int))
                            l.extend(y_val_data.numpy().astype(int))
                    inner_fold_acc.append(accuracy_score(l, p))
                
                mean_inner_acc = np.mean(inner_fold_acc)
                if mean_inner_acc > best_inner_cv_acc:
                    best_inner_cv_acc = mean_inner_acc
                    best_cfg = (lr, n_ep)
        
        print(f"  > Best HP found: lr={best_cfg[0]}, epochs={best_cfg[1]} (Inner CV Acc: {best_inner_cv_acc:.4f})")

        # --- ì™¸ë¶€ ë£¨í”„ í‰ê°€ìš© ìµœì¢… ëª¨ë¸ í›ˆë ¨ ---
        final_scaler = StandardScaler().fit(X_outer_train)
        X_outer_train_s = final_scaler.transform(X_outer_train)
        
        if len(X_outer_train_s) > 0 and len(np.unique(y_outer_train)) > 1:
            X_outer_train_sm, y_outer_train_sm = SMOTE(random_state=42).fit_resample(X_outer_train_s, y_outer_train)
        else:
            X_outer_train_sm, y_outer_train_sm = X_outer_train_s, y_outer_train

        train_ld = DataLoader(TensorDataset(t(X_outer_train_sm), t(y_outer_train_sm)), batch_size=BATCH_SIZE, shuffle=True)

        final_model = LogisticRegressionModel(input_dim).to(device)
        final_opt = optim.Adam(final_model.parameters(), lr=best_cfg[0])
        train_fixed_epochs(final_model, train_ld, final_opt, best_cfg[1])

        weights = final_model.net[0].weight.data.cpu().numpy().flatten()
        fold_weights.append(weights)
        
        # --- 'ì™¸ë¶€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸'ë¡œ ìµœì¢… í‰ê°€ ---
        X_outer_test_s = final_scaler.transform(X_outer_test)
        X_test_tensor = t(X_outer_test_s)
        
        final_model.eval()
        predicted_probs = []
        with torch.no_grad():
            # squeeze()ê°€ ìŠ¤ì¹¼ë¼ ê°’ì„ ë°˜í™˜í•˜ì§€ ì•Šë„ë¡ .numpy() ì „ì— í™•ì¸
            probs_tensor = final_model(X_test_tensor.to(device)).detach().cpu()
            if probs_tensor.dim() > 0:
                 predicted_probs = probs_tensor.squeeze().numpy()
            else: # ë°°ì¹˜ í¬ê¸° 1ì¸ ê²½ìš°
                 predicted_probs = probs_tensor.numpy().reshape(1) 
                 
        # ë§Œì•½ predicted_probsê°€ ë‹¨ì¼ ìŠ¤ì¹¼ë¼ì¸ ê²½ìš° (í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì´ 1ê°œì¼ ë•Œ)
        if not isinstance(predicted_probs, np.ndarray):
            predicted_probs = np.array([predicted_probs])
        elif predicted_probs.ndim == 0:
             predicted_probs = predicted_probs.reshape(1)
             
        predicted = (predicted_probs > 0.5).astype(int)

        # ì§€í‘œ ê³„ì‚°
        cm = confusion_matrix(y_outer_test, predicted)
        if cm.size == 4: # 2x2 ë§¤íŠ¸ë¦­ìŠ¤ì¼ ë•Œë§Œ
            TN, FP, FN, TP = cm.ravel()
            sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0
            specificity = TN / (TN + FP) if (TN + FP) != 0 else 0
        else: # í•œ í´ë˜ìŠ¤ë§Œ ì˜ˆì¸¡/ì‹¤ì œì¸ ê²½ìš°
            sensitivity = 0
            specificity = 0
            
        f1 = f1_score(y_outer_test, predicted)
        accuracy = accuracy_score(y_outer_test, predicted)
        
        auroc = np.nan
        if len(np.unique(y_outer_test)) > 1:
            auroc = roc_auc_score(y_outer_test, predicted_probs)
            ci_low, ci_high = calculate_ci(y_outer_test, predicted_probs)
            
            # --- [ë°ì´í„° ìˆ˜ì§‘ 2] ROC Curve Interpolation ---
            fpr, tpr_fold, _ = roc_curve(y_outer_test, predicted_probs)
            interp_tpr = np.interp(mean_fpr, fpr, tpr_fold)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
        else:
            auroc = np.nan; ci_low, ci_high = 0.0, 0.0
        
        # ì ìˆ˜ ì €ì¥
        outer_test_accuracy.append(accuracy)
        outer_test_auroc.append(auroc)
        outer_test_sensitivity.append(sensitivity)
        outer_test_specificity.append(specificity)
        outer_test_f1.append(f1)
        
        print(f"  > Fold {fold_idx + 1} AUROC: {auroc:.4f} (95% CI: {ci_low:.3f}-{ci_high:.3f})")

      
    # --- 6. ëª¨ë¸ë³„ ìµœì¢… ê²°ê³¼ ë³´ê³  (Mean Â± Std) ---
    print("\n" + "-" * 60)
    print(f"ğŸ“Š [Result] {model_name} (Nested 5-Fold CV)")
    print(f"AUROC:     {np.nanmean(outer_test_auroc):.4f} Â± {np.nanstd(outer_test_auroc):.4f}")
    print(f"Accuracy:  {np.mean(outer_test_accuracy):.4f} Â± {np.std(outer_test_accuracy):.4f}")
    print(f"Sensitivity: {np.mean(outer_test_sensitivity):.4f} Â± {np.std(outer_test_sensitivity):.4f}")
    print(f"Specificity: {np.mean(outer_test_specificity):.4f} Â± {np.std(outer_test_specificity):.4f}")
    print(f"F1 Score:    {np.mean(outer_test_f1):.4f} Â± {np.std(outer_test_f1):.4f}")
    print("=" * 60)

# [Visualization 1] ROC Curve with Mean & Std
    if len(tprs) > 0:
        valid_aucs = [a for a in outer_test_auroc if not np.isnan(a)]
        plot_mean_roc_with_std(tprs, valid_aucs, model_name)
        plt.show()
    
    # [Visualization 2] Odds Ratio with Mean & Std
    if fold_weights:
        plot_odds_ratio_with_std(fold_weights, current_features, model_name)
