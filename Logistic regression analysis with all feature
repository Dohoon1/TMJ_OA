import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import numpy as np
import random

xlsx_path = '/content/TMJ_OA_MRI_data.xlsx' 

df = pd.read_excel(xlsx_path).dropna()
df.rename(columns={'PA_TMJ_OA': 'PR_TMJ_OA'}, inplace=True)
df = df.drop(columns=['ID', 'Pain side'])

# Seed
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if using multi-GPU
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True  # for reproducibility
    torch.backends.cudnn.benchmark = False

set_seed(42)

features = [ 'Sex', 'Age',	'VAS', 'Symptom duration', 'TMJ noise',	'Muscle stiffness',	'Locking',	'Bruxism', 'MRI_TMJ_ADD',	'PR_TMJ_OA', 'NaMx_Discrepancy', 'MxMn_Discrepancy']
target = ['MRI_TMJ_OA']

X = df.drop(columns=['MRI_TMJ_OA']).values
y = df['MRI_TMJ_OA'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42)

LR_LIST     = [0.05, 0.01, 0.005, 0.001]
EPOCH_LIST  = [500, 1000, 1500, 2000]          # Early-Stop ÏóÜÏùå
BATCH_SIZE  = 128
K           = 5                            # k-fold

class LogisticRegressionModel(nn.Module):
    def __init__(self, in_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(in_dim, 1), nn.Sigmoid())
    def forward(self, x): return self.net(x)

device   = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
criterion = nn.BCELoss()

def accuracy(model, loader):
    model.eval(); p, l = [], []
    with torch.no_grad():
        for x, y in loader:
            out = model(x.to(device)).squeeze().cpu().numpy()
            p.extend((out > 0.5).astype(int)); l.extend(y.numpy().astype(int))
    return accuracy_score(l, p)

def train_fixed_epochs(model, loader, optimizer, n_epochs):
    model.train()
    for _ in range(n_epochs):
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            loss = criterion(model(x).squeeze(), y)
            loss.backward(); optimizer.step()

skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)
input_dim = X_train.shape[1]
best_cfg, best_cv_acc = None, -1.0

for lr in LR_LIST:
    for n_ep in EPOCH_LIST:
        fold_acc = []
        for tr_idx, val_idx in skf.split(X_train, y_train):
            # (1) Ìè¥Îìú Î∂ÑÎ¶¨
            X_tr, X_val = X_train[tr_idx], X_train[val_idx]
            y_tr, y_val = y_train[tr_idx], y_train[val_idx]

            # (2) Ïä§ÏºÄÏùºÎßÅÍ≥º SMOTE  (ÌõàÎ†® fold Î°úÎßå fit)
            scaler = StandardScaler().fit(X_tr)
            X_tr_s = scaler.transform(X_tr)
            X_val_s= scaler.transform(X_val)
            X_tr_sm, y_tr_sm = SMOTE(random_state=42).fit_resample(X_tr_s, y_tr)

            # (3) Tensor & DataLoader
            t = lambda a: torch.tensor(a, dtype=torch.float32)
            train_ld = DataLoader(TensorDataset(t(X_tr_sm), t(y_tr_sm)),
                                  batch_size=BATCH_SIZE, shuffle=True)
            val_ld   = DataLoader(TensorDataset(t(X_val_s),  t(y_val)),
                                  batch_size=BATCH_SIZE)

            # (4) Î™®Îç∏ ÌïôÏäµ
            model = LogisticRegressionModel(input_dim).to(device)
            opt   = optim.Adam(model.parameters(), lr=lr)
            train_fixed_epochs(model, train_ld, opt, n_ep)

            # (5) Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ
            fold_acc.append(accuracy(model, val_ld))

        mean_acc = np.mean(fold_acc)
        print(f'lr={lr:<6}  ep={n_ep:<3} | Mean acc ={mean_acc:.4f}')
        if mean_acc > best_cv_acc:
            best_cv_acc = mean_acc
            best_cfg    = (lr, n_ep)

print(f'\nüèÜ  Best hyper-params  ‚Üí  lr={best_cfg[0]}, epochs={best_cfg[1]} '
      f'(CV acc={best_cv_acc:.4f})')


scaler = StandardScaler().fit(X_train)
X_train_s = scaler.transform(X_train)
X_train_sm, y_train_sm = SMOTE(random_state=42).fit_resample(X_train_s, y_train)

train_ld = DataLoader(TensorDataset(
    torch.tensor(X_train_sm, dtype=torch.float32),
    torch.tensor(y_train_sm, dtype=torch.float32)),
    batch_size=BATCH_SIZE, shuffle=True)

final_model = LogisticRegressionModel(input_dim).to(device)
final_opt   = optim.Adam(final_model.parameters(), lr=best_cfg[0])
train_fixed_epochs(final_model, train_ld, final_opt, best_cfg[1])


X_test_s = scaler.transform(X_test)
test_ld  = DataLoader(TensorDataset(
    torch.tensor(X_test_s, dtype=torch.float32),
    torch.tensor(y_test,   dtype=torch.float32)),
    batch_size=BATCH_SIZE)

print('üìä  Test accuracy =', accuracy(final_model, test_ld))

import matplotlib.pyplot as plt

with torch.no_grad():
    # fc Î†àÏù¥Ïñ¥Ïùò weightÎäî (1, input_size) ÌòïÌÉúÏûÖÎãàÎã§.
    coefficients = final_model.net[0].weight.data.cpu().numpy()[0]  # shape: (input_size,)
    # odds ratioÎäî Í∞Å Í≥ÑÏàòÏùò ÏßÄÏàò Î≥ÄÌôòÏúºÎ°ú Í≥ÑÏÇ∞Ìï©ÎãàÎã§.
    odds_ratio = np.exp(coefficients)

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(17, 6))

# 1. Coefficient
bars0 = axes[0].barh(features, coefficients, color='skyblue')
axes[0].set_title('Feature Coefficients')
axes[0].set_xlabel('Coefficient Value')
axes[0].set_ylabel('Feature')

# 2. Odds Ratio
bars1 = axes[1].barh(features, odds_ratio, color='salmon')
axes[1].set_title('Feature Odds Ratios')
axes[1].set_xlabel('Odds Ratio')
axes[1].set_ylabel('Feature')

for bar in bars0:
    width = bar.get_width()
    # ÎßâÎåÄ Ï§ëÏïô ÏúÑÏπòÏóê ÏàòÏπòÍ∞íÏùÑ ÌëúÏãúÌï©ÎãàÎã§.
    axes[0].text(width, bar.get_y() + bar.get_height()/2, f'{width:.2f}',
                 va='center', ha='left', fontsize=10, color='black')

for bar in bars1:
    width = bar.get_width()
    axes[1].text(width, bar.get_y() + bar.get_height()/2, f'{width:.2f}',
                 va='center', ha='left', fontsize=10, color='black')

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    confusion_matrix,
    f1_score,
    accuracy_score,
    roc_curve,
    roc_auc_score,
    auc
)
from sklearn.utils import resample
X_test_s = torch.tensor(X_test_s, dtype=torch.float32)
predicted_probs = final_model(X_test_s.to(device)).detach().cpu().numpy().squeeze()
predicted = (predicted_probs > 0.5).astype(int)

# confusion matrix
cm = confusion_matrix(y_test, predicted)
TN, FP, FN, TP = cm.ravel()

# Sensitivity (Recall), Specificity, F1 score, Accuracy
sensitivity = TP / (TP + FN) if (TP+FN) != 0 else 0
specificity = TN / (TN + FP) if (TN+FP) != 0 else 0
f1 = f1_score(y_test, predicted)
accuracy = accuracy_score(y_test, predicted)
auroc = roc_auc_score(y_test, predicted_probs)

# ROC curve
fpr, tpr, thresholds = roc_curve(y_test, predicted_probs)
roc_auc = auc(fpr, tpr)

# AUROC 95% CI using bootstraping
n_bootstraps = 1000
rng = np.random.RandomState(42)
bootstrapped_scores = []
for i in range(n_bootstraps):
    indices = rng.randint(0, len(y_test), len(y_test))
    if len(np.unique(y_test[indices])) < 2:
        continue  
    score = roc_auc_score(y_test[indices], predicted_probs[indices])
    bootstrapped_scores.append(score)

bootstrapped_scores = np.array(bootstrapped_scores)
bootstrapped_scores.sort()

ci_lower = bootstrapped_scores[int(0.025 * len(bootstrapped_scores))]
ci_upper = bootstrapped_scores[int(0.975 * len(bootstrapped_scores))]

print("Sensitivity (Recall): {:.4f}".format(sensitivity))
print("Specificity: {:.4f}".format(specificity))
print("F1 Score: {:.4f}".format(f1))
print("Accuracy: {:.4f}".format(accuracy))
print("AUROC: {:.4f}".format(auroc))
print("95% CI for AUROC: [{:.4f}, {:.4f}]".format(ci_lower, ci_upper))

# ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='grey', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for logistic regression')
plt.legend(loc="lower right")
plt.show()
